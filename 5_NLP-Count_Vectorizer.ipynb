{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and \"Count Vectorizing\" Text\n",
    "#### In this notebook, I will process the text data to each movie, doing final cleaning of punctuation, making all words lowercase and spliting text into individual word \"tokens\" (N.B., for those who are text processing-savvy, this was done outside of Vectorizers in order to retain the '-', since curse words are represented in these reviews as s--t and f--k. The frequency with which curse words appear in reviews may be an important text feature for parents...).\n",
    "\n",
    "#### The first vectorizer I will explore is Count Vectorizer. Count Vectorizer will take the simple frequency of words found in text associated with each movie and turn it into that movie's \"word vector.\" Movies with similar word vectors will be judged to be similar. To favor words that appear to be associated with movies in general and to reduce the dimensionality of our word vector space, I will run the word vectors through a process called Truncated SVD. Truncated SVD is a method designed to capture the most variance in our collected movie word vectors in the fewest dimensions possible by taking orthogonal components through our vector space, each of which is a combination of words. I will then analyze these components to see if I can derive some meaning for some of them (see below).\n",
    "\n",
    "#### Once we get our text data into truncatedSVD format, we will then use cosine similarity to determine which movies are most similar to which other movies in our data set. We will then also incorporate non-text data to see how this improves our cosine similarity-based similarity matrix (see Notebook 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Movie Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukiharuhadeishi/anaconda3/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, re, json, copy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json of movies_features_text\n",
    "with open('data/movies_features_text.json') as json_file:  \n",
    "    movies_features_text = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>slug</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sicario-day-of-the-soldado</td>\n",
       "      <td>Families can talk about Sicario: Day of the So...</td>\n",
       "      <td>Sicario: Day of the Soldado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>damsel</td>\n",
       "      <td>Families can talk about Damsel  use of violenc...</td>\n",
       "      <td>Damsel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>distorted</td>\n",
       "      <td>Families can talk about the rapid-fire disturb...</td>\n",
       "      <td>Distorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the-catcher-was-a-spy</td>\n",
       "      <td>Families can talk about Berg  sexual orientati...</td>\n",
       "      <td>The Catcher Was a Spy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>boundaries</td>\n",
       "      <td>Families can talk about how Boundaries portray...</td>\n",
       "      <td>Boundaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                        slug  \\\n",
       "0         0  sicario-day-of-the-soldado   \n",
       "1         1                      damsel   \n",
       "2         2                   distorted   \n",
       "3         3       the-catcher-was-a-spy   \n",
       "4         4                  boundaries   \n",
       "\n",
       "                                                text  \\\n",
       "0  Families can talk about Sicario: Day of the So...   \n",
       "1  Families can talk about Damsel  use of violenc...   \n",
       "2  Families can talk about the rapid-fire disturb...   \n",
       "3  Families can talk about Berg  sexual orientati...   \n",
       "4  Families can talk about how Boundaries portray...   \n",
       "\n",
       "                         title  \n",
       "0  Sicario: Day of the Soldado  \n",
       "1                       Damsel  \n",
       "2                    Distorted  \n",
       "3        The Catcher Was a Spy  \n",
       "4                   Boundaries  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(movies_features_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_list(last_movie):\n",
    "    for movie_num in range(last_movie):\n",
    "        movie_titles = movies_features_text[movie_num]['title']\n",
    "        return movie_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Families can talk about Sicario: Day of the Soldado  violence. Which parts were gruesome, and which were exciting? How did the movie achieve these effects? What  the impact of media violence on kids?  How are drinking, smoking, and drugs depicted? Are they glamorized? Does the movie make the drug business look alluring?  What does the movie have to say about law versus justice?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_features_text[0]['text'][:380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "# Remove punctuation from all text of each movie and remove stopwords\n",
    "def clean_text_for_movie(text):\n",
    "    '''\n",
    "    Takes in all text of a single movie, makes lowercase and removes punctuation and stopwords\n",
    "    from text. Returns words in input text as a single string, w/o English stopwords.\n",
    "    '''\n",
    "    words = re.sub(\"[^a-zA-Z\\-]\", \" \", text).lower().split()  # removes punctuation, makes lowercase\n",
    "    cleantext = [w for w in words if not w in stopwords]  # eliminates common \"stop words\"\n",
    "    return(\" \".join(cleantext))  # returns words as a string, each word separated by a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_test = clean_text_for_movie(movies_features_text[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'families talk sicario day soldado violence parts gruesome exciting movie achieve effects impact media violence kids drinking smoking drugs depicted glamorized movie make drug business look alluring movie say law versus justice difference two end justify means'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_test[:259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "def clean_text_for_movies(first_movie, num_movies_to_clean):\n",
    "    print(\"Number of movies cleaned so far:\")\n",
    "    for movie in range(num_movies_to_clean):\n",
    "        movie = (movie + first_movie)\n",
    "        if movie % 1000 == 0:\n",
    "            print(movie)\n",
    "        clean_txt = clean_text_for_movie(movies_features_text[movie]['text'])\n",
    "        movies_features_text[movie]['clean_text'] = clean_txt\n",
    "        clean_text.append(clean_txt)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies cleaned so far:\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "clean_text = clean_text_for_movies(0,len(movies_features_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>slug</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>families talk sicario day soldado violence par...</td>\n",
       "      <td>0</td>\n",
       "      <td>sicario-day-of-the-soldado</td>\n",
       "      <td>Families can talk about Sicario: Day of the So...</td>\n",
       "      <td>Sicario: Day of the Soldado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>families talk damsel use violence intense freq...</td>\n",
       "      <td>1</td>\n",
       "      <td>damsel</td>\n",
       "      <td>Families can talk about Damsel  use of violenc...</td>\n",
       "      <td>Damsel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>families talk rapid-fire disturbing images dis...</td>\n",
       "      <td>2</td>\n",
       "      <td>distorted</td>\n",
       "      <td>Families can talk about the rapid-fire disturb...</td>\n",
       "      <td>Distorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>families talk berg sexual orientation presente...</td>\n",
       "      <td>3</td>\n",
       "      <td>the-catcher-was-a-spy</td>\n",
       "      <td>Families can talk about Berg  sexual orientati...</td>\n",
       "      <td>The Catcher Was a Spy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>families talk boundaries portrays drugs drug u...</td>\n",
       "      <td>4</td>\n",
       "      <td>boundaries</td>\n",
       "      <td>Families can talk about how Boundaries portray...</td>\n",
       "      <td>Boundaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  movie_id  \\\n",
       "0  families talk sicario day soldado violence par...         0   \n",
       "1  families talk damsel use violence intense freq...         1   \n",
       "2  families talk rapid-fire disturbing images dis...         2   \n",
       "3  families talk berg sexual orientation presente...         3   \n",
       "4  families talk boundaries portrays drugs drug u...         4   \n",
       "\n",
       "                         slug  \\\n",
       "0  sicario-day-of-the-soldado   \n",
       "1                      damsel   \n",
       "2                   distorted   \n",
       "3       the-catcher-was-a-spy   \n",
       "4                  boundaries   \n",
       "\n",
       "                                                text  \\\n",
       "0  Families can talk about Sicario: Day of the So...   \n",
       "1  Families can talk about Damsel  use of violenc...   \n",
       "2  Families can talk about the rapid-fire disturb...   \n",
       "3  Families can talk about Berg  sexual orientati...   \n",
       "4  Families can talk about how Boundaries portray...   \n",
       "\n",
       "                         title  \n",
       "0  Sicario: Day of the Soldado  \n",
       "1                       Damsel  \n",
       "2                    Distorted  \n",
       "3        The Catcher Was a Spy  \n",
       "4                   Boundaries  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(movies_features_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movies_features_text now has two new features, one of which contains the list of words used in movie reviews and other text associated with each of our 8625 unique movies and a second list that contains a list of bigrams of these words, to capture names of actors, separated by sentence. movies_fetures_text is now ready for vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text for NLP\n",
    "#### I will initially use a tool called Count Vectorizer to establish an easily interpreted simple count of unigram and bigram frequency in my dataset. I will evaluate the predictive value of Count Vectorization before and after combining it with my Non-Text Features (see Notebook 7).\n",
    "#### I will also use a process called TF-IDF (Term Frequency Inverse Document Frequency) Vectorization on my text data for comparison. TF-IDF gives the frequency of each word in the words associated with each movie (termed a \"document\") normalized by the frequency with which that word appears in all of the documents combined.\n",
    "#### After Count or TD-IDF vectorization, I will then use truncated SVD on text data alone to reduce the number of features to reduce overfitting. The components that result from truncated SVD will be examined to identify discernable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(analyzer = \"word\",\n",
    "                      tokenizer = None,      # tokenized in preprocessing\n",
    "                      preprocessor = None,\n",
    "                      stop_words = None,     # english stop words already removed, to retain -\n",
    "                      min_df = 2,            # to eliminate typos\n",
    "                      max_df = .9,           # to eliminate the word \"movie\"\n",
    "                      max_features = 100000) \n",
    "\n",
    "data_features = pd.SparseDataFrame(vec.fit_transform(clean_text),\n",
    "                                   columns=vec.get_feature_names(),\n",
    "                                   default_fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8625, 42188)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape   # unigrams only (far too many features with larger ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code bits for Future Pre-Modeling and Analysis\n",
    "### Investigative EDA - Pre-Modeling\n",
    "    Things to look for:\n",
    "      Global token counts\n",
    "      Select words of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(min_df=2, max_df=.9)  # min_df = 2 eliminates typos,\n",
    "#             # max_df = .9 eliminates the word \"movie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Vectorized Data in DF for Analysis\n",
    "- Sum aggregate token counts\n",
    "- Plot / investigate\n",
    "  - Histogram\n",
    "  - Horizontal Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_text = text.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_text.sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_text.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = sum_text.between(3000, 15000)\n",
    "# sum_text[mask].sort_values(ascending=False).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sum_text[mask].sort_values(ascending=True).head(20).plot(kind=\"barh\", figsize=(5, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8625, 42188)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape  # unigrams only (far too many features with larger ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pd.DataFrame(data_features.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "#### This vectorizer will vectorizes words in text by count, normalizing word frequency by including a factor that will decrease the effect of commonly occuring words specific to the posts analyzed. This is done for reasons similiar to why we discount words that occur frequently in the English language generally--- their appearance may obscure more important, differentiating words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvec = TfidfVectorizer(analyzer = \"word\",\n",
    "#                        tokenizer = None,      # tokenized in preprocessing\n",
    "#                        preprocessor = None,\n",
    "#                        stop_words = None,     # english stop words already removed, to retain -\n",
    "#                        min_df = 2,            # to eliminate typos\n",
    "#                        max_df = .9,           # to eliminate the word \"movie\"\n",
    "#                        max_features = 42164) \n",
    "# \n",
    "# data_features_tfidf = pd.SparseDataFrame(tvec.fit_transform(clean_text),\n",
    "#                                           columns=tvec.get_feature_names(),\n",
    "#                                           default_fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = tvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Consider stemming, to avoid 'abandon', 'abandoned', 'abandoning', 'abandonment',\n",
    "### and 'abandons' all ending up as separate words, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### investigate top word choices--- how to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "#### To generate vectors that encapsulate the most variance in our text data in the fewest number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment cells below and run, to find TruncatedSVD1000 results for CountVectorized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countvec_truncated is fit_transformed w/1000 components\n",
    "countvec_truncated = svd.fit_transform(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " countvec_truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " components_countvec = pd.DataFrame(svd.components_.T, index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " components_countvec   # columns are svd components, 0 - 999, for count vectorized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " components_countvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_truncated = svd.fit_transform(data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components_tfidf = pd.DataFrame(svd.components_.T, index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components_tfidf   # columns are svd components, 0 - 999, for tf-idf vectorized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore components--- list actual numerical value, but print head(10) and tail(10) for important components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_importance_by_component = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_importance_component_1 = components_tfidf[0].abs()\n",
    "# word_importance_component_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(word_importance_component_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_importance_component_1.index   ### Take word order from abs(), but list actual value!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_importances_tfidf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd_explained_variance = svd.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_explained_variance = []\n",
    "def cum_sum_explained_var(vect, total_comp):\n",
    "    cum_sum_explained_variance = []\n",
    "    if total_comp > len(vect.explained_variance_):\n",
    "        print(\"That's too many components. Max_components is 1000\\.\\n\")\n",
    "        total_comp = int(input(\"Enter new total_components:\"))\n",
    "    else:\n",
    "        pass\n",
    "    cum_sum_var = 0\n",
    "    for i in range(total_comp):\n",
    "        cum_sum_var += vect.explained_variance_[i]\n",
    "        cum_sum_explained_variance.append(cum_sum_var)\n",
    "    return cum_sum_explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cum_sum_explained_var(svd, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_features_tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pd.DataFrame(index=feature_names, columns=components_tfidf)\n",
    "## tfidf_features['feature names'] = vocab\n",
    "## #pd.DataFrame(features_components_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity:  TFIDF_truncSVD700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calcualte as matrix of all movies to all movies of  countvec_truncated\n",
    "sim_matrix_countvec_truncSVD700 = cosine_similarity(countvec_truncated, countvec_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_countvec_truncSVD1000 = pd.DataFrame(sim_matrix_countvec_truncSVD700,\n",
    "                                                columns=df['title'],\n",
    "                                                index=df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_countvec_truncSVD1000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_countvec_truncSVD1000['Damsel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate as matrix of all movies to all movies of tfidf_truncated \n",
    "# sim_matrix_tfidf_truncSVD700 = cosine_similarity(tfidf_truncated, tfidf_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_tfidf_truncSVD700 = pd.DataFrame(sim_matrix_tfidf_truncSVD700,\n",
    "#                                                    columns=df['title'],\n",
    "#                                                    index=df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_tfidf_truncSVD700.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_tfidf_truncSVD700['Damsel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_recommender(movie_name, movie_list, limit=3):\n",
    "    results = process.extract(movie_name, movie_list, limit=limit)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_movies():\n",
    "    movie_name = input(\"Give me a movie title and I'll give you five titles you might also like:\")\n",
    "    for title in df['title']:\n",
    "        if title == movie_name:\n",
    "            sim_movies_text = similarity_matrix_countvec_truncSVD1000[movie_name]\n",
    "            print(\"Thanks! Here are my recommendations, along with review text similarity scores:\")\n",
    "            return sim_movies_text.sort_values(ascending=False)[1:6]\n",
    "    limit = 3\n",
    "    while title != movie_name:\n",
    "        results = title_recommender(movie_name, df['title'], limit=limit)\n",
    "        print(\"Sorry, that movie title isn't in my list. Did you mean\", results, \"?\")\n",
    "        movie_name = input(\"(I need the exact title, please...)\")\n",
    "        for title in df['title']:\n",
    "            if title == movie_name:\n",
    "                sim_movies_text = similarity_matrix_countvec_truncSVD1000[movie_name]\n",
    "                print(\"Thanks! Here are my recommendations, along with review text similarity scores:\")\n",
    "                return sim_movies_text.sort_values(ascending=False)[1:6]\n",
    "        limit += 1\n",
    "        if limit >= 10:\n",
    "            limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "find_similar_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "find_similar_movies2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see above, recommendations are usually good, with some notable exceptions. Informal analyses show recommendations are accurate approximately 80-85% of the time. Unfortunately, this recommender will not work without the cosine similarity matrix, which is prohibitively large to upload to GitHub. In order to generate this file, you can either run the notebooks in this repo in order, or recompile the matrix from the 15 pieces that I've generated and uploaded as simmat0, simmat1, ..., simmat14. You can do so by executing the function at the bottom of notebook 5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
